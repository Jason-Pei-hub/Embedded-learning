# 基于yolo的手语语音识别双向沟通AR眼镜—项目架构



## 需要提前有概念的一些知识

### 服务器

可以理解为一台一直开机的电脑，能够随时接收数据并处理后给予反馈，完全可以用一台永远不关机的电脑代替

### 后端

与前端对立，前端多为页面，后端则为数据的接收和转发，一般前端的数据都会通过后端框架变为实时数据，后端可以理解为可以不断接收数据，处理后推送至前端

### 传输协议

数据的传输需要基于一定的协议，可以理解为传输数据的方法，物联网中比较常用的协议为MQTT协议和HTTP协议，根据案例学习会用即可

### 物联网

物体联网，只要是能上网的板子都叫物联网开发板，不过真实的意思肯定不是这个，能通过云端下发命令并操控或者借助互联网把本身不能处理的数据转发通过其他的算力来处理并返回结果，这个应该是我理解的物联网

### 深度学习

机器学习的一种，更像人脑处理问题的方法，深度学习的概念源于人工神经网络的研究，但是并不完全等于传统神经网络。可以理解为给出一张图片，让电脑能够通过大量的学习后认出来这是什么并返回结果



---



## 项目介绍：

可以根据需求切换两种模式，分别是听障人士端和健听人士端

#### 听障人士端：

由听障人士带AR眼镜，可以收录与之对话的人的语音，转换为文字显示在屏幕上，同时也可以解读听障人士的手语，并转化为语音播报出来

**（较难）**

**主要难点：**语音的收集识别和语音播报文字

**主要功能：**

* 语音收集并转化成文字显示在AR眼镜
* 戴眼镜者手语的识别，并把识别结果播放出来

#### 健听人士端：

由健听人士佩戴，可以实时识别并翻译手语显示在AR眼镜上

**主要难点：**手语的识别

**主要功能：**

* 能够通过摄像头捕捉对方的手语并转化为文字显示在AR眼镜上



---



## 项目架构

### 总体架构

数据采集——数据传输——云端处理——数据下放——AR显示

![image-20240616210817935](https://gitee.com/jason_pei/typora-bed/raw/master/image/202406162108081.png)

### 显示层架构

#### 设计理念

轻量化，小型化，泛用性

可以外置在日常使用眼镜的

#### 需求

能够清晰地显示文字在眼前



![0abcf4d73bf99e842678448e6fc1e79](https://gitee.com/jason_pei/typora-bed/raw/master/image/202406162144111.png)

#### 成像原理

如果距离太近眼镜会出现无法聚焦

解决方法就是使用透镜延长对焦距离

![image-20240616214907350](https://gitee.com/jason_pei/typora-bed/raw/master/image/202406162149537.png)

同时添加一块**半透半反镀层**来反射显示器的光，这样来做到屏幕和现实的叠加显示

![image-20240616215031417](https://gitee.com/jason_pei/typora-bed/raw/master/image/202406162150683.png)

#### 显示模块

使用sonyEX335显示屏＋驱动板

小尺寸 1080P分辨率

### 云端处理架构

**云服务器只用作图像识别，语音识别在本地进行**（因为有现成的硬件）

采用云服务器进行数据处理

租用华为云HECS服务器进行数据的运算

> 如果不好搭服务器可以直接用EMQX的服务器
>
> 直接接收我们发送的信息即可
>
> ![architecture_image](https://gitee.com/jason_pei/typora-bed/raw/master/image/202406162213480.png)

经过PyTorch框架中的VOLO模型处理，将接收到原始信息，处理转换为可读的文字信息。

[手势识别2：基于YOLOv5的手势识别系统(含手势识别数据集+训练代码)-CSDN博客](https://blog.csdn.net/guyuealian/article/details/126750433?ops_request_misc=%7B%22request%5Fid%22%3A%22171854795316800211551148%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fblog.%22%7D&request_id=171854795316800211551148&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-126750433-null-null.nonecase&utm_term=手语&spm=1018.2226.3001.4450)

重新下放至开发板

### 感知层架构

**视像头采集模块**

> 把采集到的图像信息能够传输到核心开发板即可

**语音识别模块**

> 能够把采集到的音频信息直接转化成文字信息发送到核心开发板

这个网上应该有很多现成的模块可以直接用

[AI智能语音识别模块（一）——离线模组介绍-CSDN博客](https://blog.csdn.net/qq_42250136/article/details/132545576?ops_request_misc=%7B%22request%5Fid%22%3A%22171854752916800188587364%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=171854752916800188587364&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-4-132545576-null-null.142^v100^pc_search_result_base2&utm_term=语音识别模块&spm=1018.2226.3001.4187)

### 传输层架构

学习传输协议

能够连接WiFi传输图像信息到服务器

可以参考[ESP8266 连接到免费的公共 MQTT 服务器 | EMQ (emqx.com)](https://www.emqx.com/zh/blog/esp8266-connects-to-the-public-mqtt-broker)

![ESP8266 连接到免费的公共 MQTT 服务器](https://gitee.com/jason_pei/typora-bed/raw/master/image/202406162223945.png)



### 核心开发板功能实现

需要使用带有操作系统能够联网实现数据上报的开发板

需要有HDMI接口

最好有一定算力，可以进行关键帧的截取，这样能够减少数据传输量，可以更快速的获得反馈

[视频提取关键帧提取_视频关键帧提取-CSDN博客](https://blog.csdn.net/Ailberty/article/details/109581016?ops_request_misc=%7B%22request%5Fid%22%3A%22171854808316800182163767%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=171854808316800182163767&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-109581016-null-null.142^v100^pc_search_result_base2&utm_term=关键帧的选取&spm=1018.2226.3001.4187)

可以安装python开发环境即可



板子待定

#### 需要的功能

* 有WIFI模块
* 有操作系统
* 有HDMI显示
* 有一定算力可以跑简单的关键帧选取算法
* 可以处理命令实现不同模式的切换